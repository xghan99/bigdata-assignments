{"cells":[{"cell_type":"markdown","source":["## Task 1: Spark SQL (15m)"],"metadata":{"id":"yvjBmGBAxnQc","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36b565b0-dc53-4172-b646-4c82e1c472be","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"id":"MkbrHZYEw5Cr","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30d54257-dc20-4174-aa40-84e1f6abc56f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sales_file_location = \"/FileStore/tables/Sales_table.csv\"\nproducts_file_location = \"/FileStore/tables/Products_table.csv\"\nsellers_file_location = \"/FileStore/tables/Sellers_table.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nproducts_table = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(products_file_location)\n\nsales_table = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(sales_file_location)\n\nsellers_table = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(sellers_file_location)"],"metadata":{"id":"2luSAeOXxBiQ","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5607f10e-0a58-4330-bbeb-fa1d6863efb1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Registering tables as SQL\nproducts_table.createOrReplaceTempView(\"product\")\nsales_table.createOrReplaceTempView(\"sale\")\nsellers_table.createOrReplaceTempView(\"sell\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3457984e-f75b-4242-bf53-6a79bc406861","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# (a) Output the top 3 most popular products sold among all sellers [2m]\n# Your table should have 1 column(s): [product_name]\n\n#T1 CTE compiles the total number of items sold per product id, and takes the top3\n#Join products table and T1 to get the names of the top 3\n\nq1a_output = spark.sql(\"\\\nWith T1 AS \\\n(SELECT product_id, sum(num_of_items_sold) as total_sold \\\nFROM sale \\\nGROUP BY product_id \\\nORDER BY total_sold DESC \\\nLIMIT 3) \\\nSELECT product_name \\\nFROM product, T1 \\\nWHERE product.product_id = T1.product_id \\\nORDER BY T1.total_sold DESC\")\n\nq1a_output.show()"],"metadata":{"id":"Ps_v7oTixnQf","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7fb33021-930c-4fa9-b595-4ed83c279ed4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n| product_name|\n+-------------+\n|product_51270|\n|product_18759|\n|product_59652|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (b) Find out the total sales of the products sold by sellers 1 to 10 and output the top most sold product [2m]\n# Your table should have 1 column(s): [product_name]\n\n#T1 CTE compiles the total number of items sold per product_id for seller_id between 1 and 10 and takes the product with the highest number sold\n#Similarly join the products table with T1 CTE to get the product name for that product id\n\nq1b_output = spark.sql(\"\\\nWith T1 AS \\\n(SELECT product_id , sum(num_of_items_sold) as total_sold \\\nFROM sale \\\nWHERE seller_id > 0 AND seller_id < 11 \\\nGROUP BY product_id \\\nORDER BY total_sold DESC LIMIT 1) \\\nSELECT product_name \\\nFROM product, T1 \\\nWHERE product.product_id = T1.product_id \\\n\")\nq1b_output.show()"],"metadata":{"id":"Ljmb_1OaxC8Q","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"866983b3-8214-4740-8f4d-90e87d1db482","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n| product_name|\n+-------------+\n|product_36658|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (c) Compute the combined revenue earned from sellers where seller_id ranges from 1 to 500 inclusive. [3m]\n# Your table should have 1 column(s): [total_revenue]\n\n#T1 CTE compiles the the total number of items sold per product_id for seller_id from 1 to 500\n#T2 CTE compiles the product_id, total number sold for this id and the price of each unit by joining T1 with the products table\n#T3 CTE compiles the total product revenue for each product_id by multiplying the price per unit to the total number of units sold\n#Final Query sums up the product revenue for all the products\nq1c_output = spark.sql(\"\\\nWith T1 AS \\\n(SELECT sale.product_id, sum(num_of_items_sold) as total_sold \\\nFROM sale \\\nWHERE sale.seller_id>0 AND sale.seller_id < 501 \\\nGROUP BY sale.product_id), \\\nT2 AS \\\n(SELECT T1.product_id, T1.total_sold, product.price \\\nFROM T1, product \\\nWHERE T1.product_id = product.product_id), \\\nT3 AS \\\n(SELECT T2.total_sold*T2.price as product_revenue \\\nFROM T2) \\\nSELECT sum(product_revenue) as total_revenue \\\nFROM T3\\\n\")\nq1c_output.show()"],"metadata":{"id":"QtinRRycxDBS","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa7bec8e-f93d-48ff-af38-d395c6fe7422","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n|total_revenue|\n+-------------+\n|    160916699|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (d) Among sellers with rating >= 4 who have achieved a combined number of products sold >= 3000, find out the top 10 most expensive product sold by any of the sellers. (If there are multiple products at the same price, please sort them in ascending order of product_id) [8m]\n# Your table should have 1 column(s): [product_name]\n# To get the full mark, your query should not run for more than 1 min\n\n#T1 compiles all the seller ids which have a seller rating >=4 and have sold >= 3000 items\n#T2 compiles the unique id, name, price of products which are sold by the seller ids in T1\n#Final Query gets the product names the top 10 most expensive products, with ties broken by product_id in ascending order\nq1d_output = spark.sql(\" \\\nWith T1 AS \\\n(SELECT sell.seller_id as id \\\nFROM sell, sale \\\nWHERE sell.seller_id = sale.seller_id AND sell.rating >= 4 \\\nGROUP BY sell.seller_id \\\nHAVING SUM(sale.num_of_items_sold) >= 3000), \\\nT2 AS \\\n(SELECT DISTINCT product.product_id, product.product_name, product.price \\\nFROM T1, sale, product \\\nWHERE T1.id = sale.seller_id AND sale.product_id = product.product_id) \\\nSELECT product_name \\\nFROM T2 \\\nORDER BY price DESC, product_id \\\nLIMIT 10 \\\n\")\nq1d_output.show()"],"metadata":{"id":"jdG80LVMxnQf","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59c00e0a-34de-4614-b783-71beb7503716","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------+\n|product_name|\n+------------+\n| product_106|\n| product_117|\n| product_363|\n| product_712|\n| product_843|\n| product_897|\n| product_923|\n|product_1466|\n|product_1507|\n|product_1514|\n+------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Task 2: Spark ML (10m)"],"metadata":{"id":"4fziMyvTxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2551ab92-377c-4492-9d99-258610b143a1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"id":"wtocOKQXxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ebc093d-9256-4e99-85d3-3d36b50a6053","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["bank_train_location = \"/FileStore/tables/bank_train.csv\"\nbank_test_location = \"/FileStore/tables/bank_test.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nbank_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(bank_train_location)\n\nbank_test = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(bank_test_location)"],"metadata":{"id":"lQB18KhnxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2eee140e-773a-4e76-9f6c-40e809e136b0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Build ML model to predict whether the customer will subscribe bank deposit service or not. Train the model using training set and evaluate the model performance (e.g. accuracy) using testing set. \n* You can explore different methods to pre-process the data and select proper features\n* You can utilize different machine learning models and tune model hyperparameters\n* Present the final testing accuracy."],"metadata":{"id":"YTZevHlAxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98477bc0-fdf9-4585-8cf2-24b4b0ebc3f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# data preparation (4m)\n#Import Packages\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml import Pipeline\n\n#Build String Indexer for Categorical Variables - Converting Categorical Features to numerical feature\ncol_to_index = [i[0] for i in bank_train.dtypes if i[1] == 'string']\nindexed_cols = [i+\"_index\" for i in col_to_index]\nstringIndexer = StringIndexer(inputCols=col_to_index, outputCols=indexed_cols)\nfitted_indexer = stringIndexer.fit(bank_train)\n\n#Transform the bank so that we can fit the one hot encoder later\nindexed_bank = fitted_indexer.transform(bank_train)\n\n#Build One-hot encoding for variables that are not binary\nbinary_indexes = ['default_index','housing_index','loan_index']\ncol_to_one_hot = [i for i in indexed_cols if i not in binary_indexes]\none_hot_output = [i+\"_hot\" for i in col_to_one_hot]\nencoder = OneHotEncoder(inputCols=col_to_one_hot,\n                        outputCols=one_hot_output)\nfitted_encoder = encoder.fit(indexed_bank)\n\n#Assembler\nto_assemble = [i[0] for i in bank_train.dtypes if i[0] not in col_to_index and i[0]!='label'] + binary_indexes +  one_hot_output\nassembler = VectorAssembler(\n    inputCols=to_assemble,\n    outputCol=\"features\")\n\n\n"],"metadata":{"id":"iey06VQfxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e07aaf5a-6fb8-425a-a3c9-f52e04e49828","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# model building (4m)\n#Build Pipeline\ngbt = GBTClassifier(maxIter = 16)\npipeline = Pipeline(stages = [fitted_indexer,fitted_encoder,assembler,gbt])\nmodel = pipeline.fit(bank_train)"],"metadata":{"id":"PsIotb9ExnQh","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a04b59e4-6197-451c-8071-52526a5a724f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# model evaluation (2m)\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\npred_test = model.transform(bank_test)\npredictionAndLabels = pred_test.select(\"prediction\",\"label\")\nevaluator = MulticlassClassificationEvaluator(metricName = \"accuracy\")\nprint(\"Test set accuracy = \"+str(evaluator.evaluate(predictionAndLabels)))"],"metadata":{"id":"OC5ufJqAxnQh","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80e1c949-8291-45be-8872-c0310777c6fa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Test set accuracy = 0.8428123600537394\n"]}],"execution_count":0}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"application/vnd.databricks.v1+notebook":{"notebookName":"cs4225_a2_databricks_student_version","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2425695814563518}},"nbformat":4,"nbformat_minor":0}
